{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning AI by Coding: Part 1-1 Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra is all about working with lines and planes, moving them around in space, stretching them or flipping them over, and seeing how these transformations interact with each other.\n",
    "\n",
    "Now, why do we need linear algebra? Here are a few reasons:\n",
    "\n",
    "1. **Solving Systems of Equations:** One of the most direct applications of linear algebra is the solution of systems of linear equations. These systems often arise naturally in various fields of science and engineering.\n",
    "\n",
    "2. **Data Representation:** In the field of computer science and data science, data is often represented in the form of vectors and matrices (which are the key elements in linear algebra). For example, images, texts, audios, and almost every other form of data can be represented as large arrays or matrices of numbers.\n",
    "\n",
    "3. **Machine Learning and Artificial Intelligence:** Linear algebra is crucial in machine learning and AI. Concepts like vector spaces and transformations are used in techniques like linear regression, logistic regression, support vector machines, neural networks, and more.\n",
    "\n",
    "4. **Computer Graphics and Animations:** Linear algebra is used in computer graphics and animations, where it's used to process and manipulate three-dimensional data.\n",
    "\n",
    "5. **Quantum Physics:** In quantum physics, the state of a quantum system is described by a vector, and observables are represented by matrices. The rules of quantum mechanics are almost entirely written in the language of linear algebra.\n",
    "\n",
    "6. **Engineering Fields:** In various engineering fields like electrical engineering and control theory, linear algebra is used to design and analyze systems and control vectors.\n",
    "\n",
    "7. **Google's Page Rank Algorithm:** Google's PageRank algorithm, which is used for web ranking, is also based on concepts from linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Python libraries for linear algebra\n",
    "\n",
    "1. **NumPy:** NumPy is a Python library used for working with arrays. It also has functions for working in the domain of linear algebra, fourier transform, and matrices. It is an essential library if you are working with numerical data in Python.\n",
    "\n",
    "2. **SciPy:** This is a powerful library for scientific computation that builds on NumPy and provides many additional features useful in linear algebra, such as functions for computing determinants, eigenvalues, matrix factorizations, and more.\n",
    "\n",
    "3. **Matplotlib:** This is a powerful plotting library for Python. It is very useful for visualizing data and mathematical concepts, which can be very helpful when learning linear algebra.\n",
    "\n",
    "4. **SymPy:** This is a Python library for symbolic mathematics. It can be used to perform algebraic computations symbolically, rather than numerically, which can be useful for understanding abstract algebraic concepts in linear algebra.\n",
    "\n",
    "5. **Pandas:** Although it's more well known for data manipulation and analysis, Pandas is also useful for handling numerical tables and time series data which can be helpful in your linear algebra journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[-3  6 -3]\n",
      "3.7416573867739413\n",
      "[[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n",
      "-2.0000000000000004\n",
      "Eigenvalues:  [-0.37228132  5.37228132]\n",
      "Eigenvectors:  [[-0.82456484 -0.41597356]\n",
      " [ 0.56576746 -0.90937671]]\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "# pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "# 1. np.dot - Dot Product\n",
    "# The dot product of two vectors is the sum of the products of the corresponding entries of the two sequences of numbers.\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "print(np.dot(a, b))  # Output: 32\n",
    "\n",
    "# 2. np.cross - Cross Product\n",
    "# The cross product of two vectors a and b is a vector that is perpendicular to both and therefore normal to the plane containing them.\n",
    "\n",
    "print(np.cross(a, b))  # Output: [-3  6 -3]\n",
    "\n",
    "# 3. np.linalg.norm - Vector Norm\n",
    "# The norm of a vector (quantity that in rough terms measures the length, size, or magnitude of a vector\n",
    "\n",
    "print(np.linalg.norm(a))  # Output: 3.7416573867739413\n",
    "\n",
    "# 4. np.linalg.inv - Inverse of a Matrix\n",
    "# The inverse of a matrix A,  denoted as A^-1, such that when you multiply A and A^-1, you get the identity matrix I.\n",
    "# In other words, for a square matrix A ==> A * A^-1 = A^-1 * A = I\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(A_inv)\n",
    "# Output:\n",
    "# [[-2.   1. ]\n",
    "#  [ 1.5 -0.5]]\n",
    "\n",
    "# 5. np.linalg.det - Determinant of a Matrix\n",
    "# The determinant is a special number associated with a square matrix (a matrix with the same number of rows and columns). \n",
    "# The determinant provides important information about the matrix and the linear map it represents.\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "print(np.linalg.det(A))  # Output: -2.0000000000000004\n",
    "\n",
    "# 6. np.linalg.eig - Eigenvalues and Eigenvectors of a Matrix\n",
    "# In linear algebra, an eigenvector or characteristic vector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it.\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "print(\"Eigenvalues: \", eigenvalues)\n",
    "print(\"Eigenvectors: \", eigenvectors)\n",
    "\n",
    "# 7. np.matmul - Matrix Multiplication\n",
    "# Matrix multiplication, also known as matrix product, that produces a single matrix through the multiplication of two different matrices.\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "print(np.matmul(A, B))\n",
    "# Output:\n",
    "# [[19 22]\n",
    "#  [43 50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "- Markdown: \n",
    "    - https://www.markdownguide.org/basic-syntax/\n",
    "    - https://wordpress.com/support/markdown-quick-reference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why need inverse matrix?\n",
    "\n",
    "The inverse of a matrix is a key concept in linear algebra and is used extensively in several areas of mathematics, computer science, and engineering. Here are a few examples of why we might need the inverse of a matrix:\n",
    "\n",
    "1. Solving Systems of Linear Equations: One of the primary uses of matrix inverses is to solve systems of linear equations. If you have a system of linear equations that can be expressed in the form Ax = b, where A is the matrix of coefficients, x is the vector of variables, and b is the constant vector, then the solution to the system can be found (if A is invertible) using the inverse of A, as x = A^-1 * b.\n",
    "\n",
    "2. Finding the Original Matrix After a Transformation: If a matrix is used to perform a transformation on a set of vectors (like rotation, scaling, or shearing), the inverse of that matrix can be used to reverse the transformation and return the vectors to their original position.\n",
    "\n",
    "3. Machine Learning and Statistics: In many machine learning and statistical models, we often need to find parameters that minimize a certain loss function. For example, in linear regression, the optimal parameters can be computed directly if the matrix of predictors is invertible, using the formula (X^TX)^-1 * X^T * y, where X is the matrix of predictors, y is the target vector, and (X^TX)^-1 * X^T is the Moore-Penrose pseudoinverse of X.\n",
    "\n",
    "4. Computer Graphics: In computer graphics, matrices are used to perform transformations such as rotation, scaling, and translation. The inverse of a transformation matrix can be used to reverse a transformation, which is useful in many applications such as animation, game physics, and ray tracing in 3D rendering.\n",
    "\n",
    "5. Control Theory: In control theory, the inverse of a matrix is used in the design of systems that can maintain stability in response to external inputs.\n",
    "\n",
    "Please note, however, that finding the inverse of a matrix can be computationally expensive for large matrices, and in many practical cases, other methods (like numerical approximation methods) may be used instead.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
